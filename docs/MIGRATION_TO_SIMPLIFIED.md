# Миграция на упрощённую архитектуру

## Контекст

Текущая реализация LifeMart_Safety демонстрирует высокую точность классификации (ROC-AUC 0.998 на валидационной выборке) за счёт использования контекстных эмбеддингов и мета-классификатора на 41 признаке. Однако архитектура обладает существенными операционными ограничениями, препятствующими широкому применению в production-окружении.

## Проблемы текущей реализации

### Латентность
Среднее время обработки одного сообщения составляет 850ms, из которых 800-1000ms приходится на три последовательных вызова Ollama API для получения эмбеддингов (E_msg, E_ctx, E_user). При нагрузке более 50-70 сообщений в минуту формируется очередь, приводящая к задержкам доставки уведомлений модераторам.

### Внешние зависимости
Система требует запущенного Ollama сервера с загруженной моделью multilingual-e5-small (~400MB параметров, 2GB RAM в рантайме). Отсутствие GPU увеличивает время генерации одного эмбеддинга до 2-3 секунд. При недоступности Ollama система деградирует до базовых фильтров (Keyword + TF-IDF), что снижает precision на 15-20%.

### Операционная сложность
Требуется поддержка пяти артефактов машинного обучения:
- `centroids.npz` — центроиды классов spam/ham
- `prototypes.npz` — 11 прототипов семейств (7 spam, 4 legit)
- `meta_model.joblib` — LogisticRegression
- `meta_calibrator.joblib` — CalibratedClassifierCV
- `feature_spec.json` — спецификация порядка признаков

Обучение мета-классификатора требует предварительной генерации всех эмбеддингов для обучающей выборки, что занимает 30-60 минут на CPU при размере датасета 5000+ примеров.

### Архитектурные ограничения
Конфигурация не поддерживает несколько чатов без значительной модификации. Глобальные настройки (MODERATOR_CHAT_ID, WHITELIST_USER_IDS, пороги) жёстко привязаны к единственному сообществу. История пользователей хранится глобально (по user_id), что приводит к смешиванию контекста при активности пользователя в нескольких чатах.

## Упрощённая архитектура

### Принципиальная схема

```
Message → KeywordFilter → TF-IDF (char-grams) → PatternClassifier (LightGBM) → PolicyEngine → Action
            <1ms            3-6ms                    2-3ms                         <1ms
                                                                                          
Total latency: <10ms (85x faster)
```

### Удаляемые компоненты
- Ollama API и все вызовы эмбеддингов
- EmbeddingFilter и провайдеры (OllamaProvider, MistralProvider)
- Центроиды и прототипы семейств
- Контекстные капсулы (E_ctx, E_user)
- LRU-кэш для истории пользователей
- Graceful degradation логика

### Модифицируемые компоненты

**TF-IDF Filter:**
- Добавление char-level n-grams (2-4) для устойчивости к обфускации
- Замена MultinomialNB на LogisticRegression для лучшей калибровки
- Увеличение vocabulary size до 10000 токенов

**Pattern Classifier (новый компонент):**
- Входные признаки: 20 (вместо 41)
  - Scores фильтров: keyword_score, tfidf_score (2)
  - Паттерны текста: has_phone, has_url, has_email, has_money, money_count, has_age, has_cta_plus, has_dm, has_remote, has_legal, has_casino, obfuscation_ratio (12)
  - Контекстные флаги: reply_to_staff, is_forwarded, author_is_admin, is_channel_announcement (4)
  - Whitelist хиты: store, order, brand (3)
- Модель: LightGBM (100 деревьев, max_depth=5)
- Калибровка через isotonic regression

**PolicyEngine:**
- Сохранение режимов (manual, semi-auto, auto)
- Сохранение понижающих множителей
- Упрощение логики (отсутствие degradation handling)

## Ожидаемые метрики

### Прогноз качества
На основе экспериментов с TF-IDF + паттерн-признаками без эмбеддингов:
```
Precision:     0.88-0.90 (vs 0.99)
Recall:        0.86-0.88 (vs 0.99)
F1-score:      0.87-0.89 (vs 0.99)
ROC-AUC:       0.91-0.93 (vs 0.998)
FP rate:       0.05-0.07 (vs 0.008)
FN rate:       0.12-0.14 (vs 0.007)
```

Снижение ROC-AUC на 0.06-0.08 обусловлено отсутствием семантической информации. Основные потери:
- Парафразы и обфускация спама (замена "заработок" на "з@р@б0т0к")
- Новые семейства спама, семантически похожие на известные, но с другими ключевыми словами
- Учёт истории поведения пользователя (ранее легитимный → стал спамить)

### Латентность
```
Keyword:         <1ms
TF-IDF:          3-6ms (char-grams увеличивают время)
Pattern LGBM:    2-3ms
Policy:          <1ms
Total:           <10ms (p95 < 15ms)
```

### Ресурсы
```
RAM (runtime):   100-200MB (vs 500-800MB)
CPU (inference): <5% одного ядра (vs 40-60%)
Disk:            15MB артефактов (vs 120MB)
```

## План миграции

### Этап 1: Подготовка обучающих данных
1. Экспорт текущего датасета `data/messages.csv` (message, label)
2. Удаление дубликатов по hash(message)
3. Проверка баланса классов (рекомендуется 40-60% spam)
4. Разделение на train/validation (80/20) с GroupKFold по user_id при наличии

### Этап 2: Обучение моделей
1. TF-IDF + LogReg:
   ```python
   TfidfVectorizer(
       max_features=10000,
       ngram_range=(1, 3),
       analyzer='char_wb',
       sublinear_tf=True
   )
   LogisticRegression(C=1.0, max_iter=1000, class_weight='balanced')
   ```
   
2. Pattern Classifier:
   ```python
   LGBMClassifier(
       n_estimators=100,
       max_depth=5,
       learning_rate=0.1,
       num_leaves=31,
       class_weight='balanced'
   )
   IsotonicRegression()  # калибровка
   ```

3. Валидация метрик (минимальные пороги):
   - ROC-AUC >= 0.88
   - Precision >= 0.85
   - Recall >= 0.83

### Этап 3: Модификация кода
1. Удаление модулей:
   - `filters/embedding.py`
   - `utils/textprep.py` (капсулы, кэш)
   - Код Ollama провайдера
   
2. Создание `filters/pattern.py`:
   ```python
   class PatternClassifier:
       def __init__(self):
           self.lgbm = load("models/pattern_lgbm.pkl")
           self.calibrator = load("models/pattern_calibrator.pkl")
       
       async def analyze(self, text: str, metadata: MessageMetadata) -> float:
           features = self._extract_features(text, metadata)
           raw_proba = self.lgbm.predict_proba(features)[0, 1]
           return self.calibrator.predict(raw_proba)
   ```

3. Упрощение `core/coordinator.py`:
   - Удаление истории чатов/пользователей
   - Удаление капсул
   - Прямой вызов фильтров без эмбеддингов

4. Обновление `services/policy.py`:
   - Удаление graceful degradation
   - Удаление режима legacy-manual (если не используется)

### Этап 4: Тестирование
1. Unit-тесты для PatternClassifier (проверка всех 20 признаков)
2. Интеграционные тесты на синтетических примерах
3. A/B тестирование на production трафике:
   - Неделя 1: параллельный запуск (старая система активна, новая логирует решения)
   - Неделя 2: новая система в режиме manual (только уведомления)
   - Неделя 3: новая система в режиме semi-auto
   - Неделя 4: полный переход на auto

### Этап 5: Мониторинг и калибровка
Метрики для отслеживания:
- FP rate (должен быть <7%)
- FN rate (должен быть <14%)
- Latency p95 (должна быть <15ms)
- CPU usage (должна быть <10%)

Корректировка порогов на основе первых 500-1000 сообщений:
```python
# Если FP rate > 7%:
META_NOTIFY += 0.05
META_DELETE += 0.05

# Если FN rate > 14%:
META_NOTIFY -= 0.03
META_DELETE -= 0.03
```

## Обратная миграция

При выявлении критических проблем (FP rate > 10% или FN rate > 20%):

1. Откат конфигурации на предыдущую версию (восстановление .env)
2. Перезапуск бота с флагом `FORCE_LEGACY_MODE=true`
3. Анализ логов для выявления паттернов ошибок
4. Дообучение моделей на новых примерах
5. Повторное тестирование перед следующей попыткой миграции

Критерий успешности миграции: стабильная работа 2 недели с FP < 7%, FN < 14%, без жалоб модераторов на качество классификации.

## Дальнейшая оптимизация

После успешной миграции возможны улучшения:

1. Добавление reranker для серых зон (0.60 < p_spam < 0.75):
   - Лёгкий cross-encoder на CPU
   - Запуск только для 5-10% сообщений
   - Уточнение финального решения

2. Онлайн-обучение:
   - Инкрементальное обновление TF-IDF векторизатора
   - Partial_fit для LogisticRegression
   - Автоматический retrain раз в неделю

3. Active learning:
   - Модератор маркирует сложные случаи
   - Приоритетное добавление в обучающую выборку
   - Переобучение при накоплении 100+ новых примеров

4. Мультичатовая архитектура:
   - Создание `config/chat_config.yaml`
   - Раздельные пороги и whitelist для каждого чата
   - Глобальная модель + чат-специфичные корректировки

## Заключение

Упрощённая архитектура жертвует 8-10% точности (ROC-AUC) в обмен на:
- Снижение латентности в 85 раз
- Устранение внешних зависимостей (Ollama)
- Снижение операционной сложности
- Готовность к горизонтальному масштабированию

Компромисс приемлем для систем с умеренными требованиями к precision (>85%) и наличием живой модерации для обработки edge cases.
